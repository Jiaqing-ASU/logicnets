{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "from dataset import JetSubstructureDataset\n",
    "from models import JetSubstructureNeqModel\n",
    "from ensemble import AveragingJetNeqModel, BaggingJetNeqModel, AdaBoostJetNeqModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = \"data/processed-pythia82-lhc13-all-pt1-50k-r1_h022_e0175_t220_nonu_truth.z\"\n",
    "dataset_config = \"config/yaml_IP_OP_config.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seeds(exp_config, cuda=False):\n",
    "    with open(exp_config, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    # Set random seeds\n",
    "    random.seed(config[\"seed\"])\n",
    "    np.random.seed(config[\"seed\"])\n",
    "    torch.manual_seed(config[\"seed\"])\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(config[\"seed\"])\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(config[\"seed\"])\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def load_dataset(dataset_file, dataset_config):\n",
    "     # Fetch the datasets\n",
    "    dataset = {}\n",
    "    dataset[\"train\"] = JetSubstructureDataset(\n",
    "        dataset_file, dataset_config, split=\"train\"\n",
    "    )\n",
    "    # This dataset is so small, we'll just use the training set as the validation set, otherwise we may have too few trainings examples to converge.\n",
    "    dataset[\"valid\"] = JetSubstructureDataset(\n",
    "        dataset_file, dataset_config, split=\"train\"\n",
    "    )\n",
    "    dataset[\"test\"] = JetSubstructureDataset(\n",
    "        dataset_file, dataset_config, split=\"test\"\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def load_model(exp_config, dataset, checkpoint=None):\n",
    "    with open(exp_config, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    # Instantiate model\n",
    "    x, y = dataset[\"train\"][0]\n",
    "    config[\"input_length\"] = len(x)\n",
    "    config[\"output_length\"] = len(y)\n",
    "    print(f\"Input length: {config['input_length']}\")\n",
    "    print(f\"Output length: {config['output_length']}\")\n",
    "\n",
    "    # Ensemble settings\n",
    "    quantize_avg = False\n",
    "    if \"quantize_avg\" in config:\n",
    "        quantize_avg = config[\"quantize_avg\"]\n",
    "    if \"post_transform_output\" not in config:\n",
    "        config[\"post_transform_output\"] = True # Default\n",
    "    if \"same_output_scale\" not in config:\n",
    "        config[\"same_output_scale\"] = False # Default\n",
    "    \n",
    "    if \"ensemble_method\" in config:\n",
    "        if config[\"ensemble_method\"] == \"averaging\":\n",
    "            print(\"Averaging ensemble method\")\n",
    "            model = AveragingJetNeqModel(\n",
    "                config, config[\"ensemble_size\"], quantize_avg=quantize_avg\n",
    "            )\n",
    "        elif config[\"ensemble_method\"] == \"bagging\":\n",
    "            print(\"Bagging ensemble method\")\n",
    "            if \"independent\" not in config:\n",
    "                config[\"independent\"] = False # Default\n",
    "            model = BaggingJetNeqModel(\n",
    "                config,\n",
    "                config[\"ensemble_size\"],\n",
    "                quantize_avg=quantize_avg,\n",
    "                single_model_mode=False,\n",
    "            )\n",
    "        elif config[\"ensemble_method\"] == \"adaboost\":\n",
    "            print(\"AdaBoost ensemble method\")\n",
    "            if \"independent\" not in config:\n",
    "                config[\"independent\"] = False # Default\n",
    "            model = AdaBoostJetNeqModel(\n",
    "                config,\n",
    "                config[\"ensemble_size\"],\n",
    "                len(dataset[\"train\"]),\n",
    "                quantize_avg=quantize_avg,\n",
    "                single_model_mode=False,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown ensemble method: {config['ensemble_method']}\")\n",
    "    else:  # Single model learning\n",
    "        model = JetSubstructureNeqModel(config)\n",
    "    if checkpoint is not None:\n",
    "        print(f\"Loading pre-trained checkpoint {checkpoint}\")\n",
    "        checkpoint = torch.load(checkpoint, map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging Sparsity Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 16\n",
      "Output length: 5\n",
      "Averaging ensemble method\n",
      "Loading pre-trained checkpoint ./averaging/averaging_small_ensemble_size2/best_accuracy.pth\n",
      "Model: AveragingJetNeqModel\n"
     ]
    }
   ],
   "source": [
    "config = \"./averaging/averaging_small_ensemble_size2/hparams.yml\"\n",
    "ckpt=\"./averaging/averaging_small_ensemble_size2/best_accuracy.pth\"\n",
    "set_random_seeds(config)\n",
    "dataset = load_dataset(dataset_file, dataset_config)\n",
    "model = load_model(config, dataset, checkpoint=ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 64, 16)\n",
      "[[[0. 1. 0. ... 1. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 1.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 1. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 1. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute Cartesian distance between input sparsity masks of the ensemble members\n",
    "def compute_cartesian_distance(model):\n",
    "    ensemble_size = model.num_models\n",
    "    input_masks = []\n",
    "    for i in range(ensemble_size):\n",
    "        mask = model.ensemble[i].module_list[0].fc.mask.mask.numpy()\n",
    "        input_masks.append(mask)\n",
    "    input_masks = np.array(input_masks)\n",
    "    print(input_masks.shape)\n",
    "    print(input_masks)\n",
    "    cartesian_distances = np.zeros((ensemble_size, ensemble_size))\n",
    "    for i in range(ensemble_size):\n",
    "        for j in range(ensemble_size):\n",
    "            cartesian_distances[i, j] = np.linalg.norm(input_masks[i] - input_masks[j])\n",
    "    total_distance = np.sum(cartesian_distances)\n",
    "    return cartesian_distances, total_distance\n",
    "dists, total_dist = compute_cartesian_distance(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.        , 17.43559647],\n",
       "        [17.43559647,  0.        ]]),\n",
       " 34.871192932128906)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists, total_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load all averaging models and compute the total Cartesian distance between their input sparsity masks\n",
    "def compute_total_cartesian_distance(exp_dir):\n",
    "    config = f\"{exp_dir}/hparams.yml\"\n",
    "    ckpt = f\"{exp_dir}/best_accuracy.pth\"\n",
    "    set_random_seeds(config)\n",
    "    dataset = load_dataset(dataset_file, dataset_config)\n",
    "    model = load_model(config, dataset, checkpoint=ckpt)\n",
    "    dists, total_dist = compute_cartesian_distance(model)\n",
    "    return total_dist\n",
    "\n",
    "# Compute for every model in the directory\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logicnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
